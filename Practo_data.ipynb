{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMc2pQAN/xDhWRiGLt0InZa",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/POOJACH76/POOJACH76/blob/main/Practo_data.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k273xFJwa2mo",
        "outputId": "409017c2-f93e-4849-ac62-ba089e4f00f1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting selenium\n",
            "  Downloading selenium-4.12.0-py3-none-any.whl (9.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.4/9.4 MB\u001b[0m \u001b[31m48.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: urllib3[socks]<3,>=1.26 in /usr/local/lib/python3.10/dist-packages (from selenium) (2.0.4)\n",
            "Collecting trio~=0.17 (from selenium)\n",
            "  Downloading trio-0.22.2-py3-none-any.whl (400 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m400.2/400.2 kB\u001b[0m \u001b[31m33.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting trio-websocket~=0.9 (from selenium)\n",
            "  Downloading trio_websocket-0.10.4-py3-none-any.whl (17 kB)\n",
            "Requirement already satisfied: certifi>=2021.10.8 in /usr/local/lib/python3.10/dist-packages (from selenium) (2023.7.22)\n",
            "Requirement already satisfied: attrs>=20.1.0 in /usr/local/lib/python3.10/dist-packages (from trio~=0.17->selenium) (23.1.0)\n",
            "Requirement already satisfied: sortedcontainers in /usr/local/lib/python3.10/dist-packages (from trio~=0.17->selenium) (2.4.0)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.10/dist-packages (from trio~=0.17->selenium) (3.4)\n",
            "Collecting outcome (from trio~=0.17->selenium)\n",
            "  Downloading outcome-1.2.0-py2.py3-none-any.whl (9.7 kB)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from trio~=0.17->selenium) (1.3.0)\n",
            "Requirement already satisfied: exceptiongroup>=1.0.0rc9 in /usr/local/lib/python3.10/dist-packages (from trio~=0.17->selenium) (1.1.3)\n",
            "Collecting wsproto>=0.14 (from trio-websocket~=0.9->selenium)\n",
            "  Downloading wsproto-1.2.0-py3-none-any.whl (24 kB)\n",
            "Requirement already satisfied: pysocks!=1.5.7,<2.0,>=1.5.6 in /usr/local/lib/python3.10/dist-packages (from urllib3[socks]<3,>=1.26->selenium) (1.7.1)\n",
            "Collecting h11<1,>=0.9.0 (from wsproto>=0.14->trio-websocket~=0.9->selenium)\n",
            "  Downloading h11-0.14.0-py3-none-any.whl (58 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m6.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: outcome, h11, wsproto, trio, trio-websocket, selenium\n",
            "Successfully installed h11-0.14.0 outcome-1.2.0 selenium-4.12.0 trio-0.22.2 trio-websocket-0.10.4 wsproto-1.2.0\n"
          ]
        }
      ],
      "source": [
        "pip install --upgrade selenium\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "from selenium import webdriver\n",
        "from bs4 import BeautifulSoup\n",
        "import pandas as pd\n",
        "\n",
        "options = webdriver.ChromeOptions()\n",
        "options.add_argument('--headless')\n",
        "options.add_argument('--no-sandbox')\n",
        "options.add_argument('--disable-dev-shm-usage')\n",
        "\n",
        "# Open it, go to a website, and get results\n",
        "wd = webdriver.Chrome(options=options)\n",
        "wd.get(\"https://www.practo.com/Bokaro/doctors\")\n",
        "time.sleep(3)  # Allow 3 seconds for the web page to open\n",
        "\n",
        "scroll_height = wd.execute_script(\"return document.body.scrollHeight\")\n",
        "new_height = 0\n",
        "\n",
        "while True:\n",
        "    wd.execute_script(\"window.scrollTo(0, document.body.scrollHeight-1500);\")  # Scroll to the last page\n",
        "    time.sleep(3)  # Scroll time\n",
        "\n",
        "    new_height = wd.execute_script(\"return document.body.scrollHeight\")\n",
        "\n",
        "    if new_height == scroll_height:\n",
        "        break  # Breaks the loop if the previous page height is the same as the new height after scrolling and refreshing\n",
        "    scroll_height = new_height  # Assigning\n",
        "\n",
        "urls = []\n",
        "link = []\n",
        "doc_name = []\n",
        "exp = []\n",
        "info = []\n",
        "fee = []\n",
        "types = []\n",
        "city = []\n",
        "local = []\n",
        "rating = []\n",
        "feedback = []\n",
        "\n",
        "soup = BeautifulSoup(wd.page_source, \"html.parser\")\n",
        "\n",
        "for parent in soup.find_all(\"div\", class_=\"u-border-general--bottom\"):\n",
        "    anchor = parent.find('a')\n",
        "    if anchor:\n",
        "        second_half = anchor.get('href')\n",
        "        link.append(\"https://www.practo.com/\" + second_half)  # Links of all doctors\n",
        "    else:\n",
        "        link.append(None)  # Or any other suitable placeholder if a link is not found\n",
        "\n",
        "    doctor_name_element = parent.select(\".doctor-name\")\n",
        "    if doctor_name_element:\n",
        "        doc_name.append(doctor_name_element[0].get_text())  # Doctor name\n",
        "    else:\n",
        "        doc_name.append(\"N/A\")  # Placeholder for missing doctor name\n",
        "\n",
        "    exp_element = parent.select(\".uv2-spacer--xs-top\")\n",
        "    if exp_element:\n",
        "        exp.append(exp_element[0].get_text().split()[0])  # Doctor experience\n",
        "    else:\n",
        "        exp.append(\"N/A\")  # Placeholder for missing experience\n",
        "\n",
        "    info_element = parent.find(\"div\", class_=\"info-section\")\n",
        "    if info_element:\n",
        "        info.append(info_element.text)\n",
        "    else:\n",
        "        info.append(\"N/A\")  # Placeholder for missing info\n",
        "\n",
        "    fee_element = parent.find('span', attrs={'data-qa-id': 'consultation_fee'})\n",
        "    if fee_element:\n",
        "        fee.append(fee_element.text)\n",
        "    else:\n",
        "        fee.append(\"N/A\")  # Placeholder for missing fee\n",
        "\n",
        "    locality_element = parent.find('span', attrs={'data-qa-id': 'practice_locality'})\n",
        "    if locality_element:\n",
        "        local.append(locality_element.text)\n",
        "    else:\n",
        "        local.append(\"N/A\")  # Placeholder for missing locality\n",
        "\n",
        "    city_element = parent.find('span', attrs={'data-qa-id': 'practice_city'})\n",
        "    if city_element:\n",
        "        city.append(city_element.text)\n",
        "    else:\n",
        "        city.append(\"N/A\")  # Placeholder for missing city\n",
        "\n",
        "    types_element = parent.find(\"div\", class_=\"u-grey_3-text\")\n",
        "    if types_element:\n",
        "        types_div = types_element.find(\"div\", class_=\"u-d-flex\")\n",
        "        if types_div:\n",
        "            types_span = types_div.span\n",
        "            if types_span:\n",
        "                types.append(types_span.text)\n",
        "            else:\n",
        "                types.append(\"N/A\")\n",
        "        else:\n",
        "            types.append(\"N/A\")\n",
        "    else:\n",
        "        types.append(\"N/A\")\n",
        "\n",
        "    rating_element = parent.find('span', attrs={'data-qa-id': 'doctor_recommendation'})\n",
        "    if rating_element:\n",
        "        rating.append(rating_element.text)\n",
        "    else:\n",
        "        rating.append(\"N/A\")  # Placeholder for missing rating\n",
        "\n",
        "    feedback_element = parent.find('span', attrs={'data-qa-id': 'total_feedback'})\n",
        "    if feedback_element:\n",
        "        feedback.append(feedback_element.text)\n",
        "    else:\n",
        "        feedback.append(\"N/A\")\n",
        "\n",
        "# Create a DataFrame from the scraped data\n",
        "data = {\n",
        "    \"Doctor Name\": doc_name,\n",
        "    \"Doctor Profile Link\": link,\n",
        "    \"Experience\": exp,\n",
        "    \"Info\": info,\n",
        "    \"Consultation Fee\": fee,\n",
        "    \"Locality\": local,\n",
        "    \"City\": city,\n",
        "    \"Types\": types,\n",
        "    \"Rating\": rating,\n",
        "    \"Feedback\": feedback\n",
        "}\n",
        "\n",
        "df = pd.DataFrame(data)\n",
        "\n",
        "# Save the DataFrame to a CSV file\n",
        "df.to_csv(\"doctors_data.csv\", index=False)\n",
        "\n",
        "# Print a message to confirm that the data has been saved\n",
        "print(\"Data has been saved to doctors_data.csv\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oa3G0ns7a3jE",
        "outputId": "6652d7fd-5118-46c7-ed8e-720d61400bda"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Data has been saved to doctors_data.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import time\n",
        "from selenium import webdriver\n",
        "from selenium.webdriver.common.by import By\n",
        "from bs4 import BeautifulSoup\n",
        "\n",
        "options = webdriver.ChromeOptions()\n",
        "options.add_argument('--headless')\n",
        "options.add_argument('--no-sandbox')\n",
        "options.add_argument('--disable-dev-shm-usage')\n",
        "\n",
        "# Open the website and allow time for the page to load\n",
        "wd = webdriver.Chrome(options=options)\n",
        "base_url = \"https://www.practo.com/delhi/doctors\"\n",
        "wd.get(base_url)\n",
        "time.sleep(3)\n",
        "\n",
        "urls = []\n",
        "link = []\n",
        "doc_name = []\n",
        "exp = []\n",
        "info = []\n",
        "fee = []\n",
        "types = []\n",
        "city = []\n",
        "local = []\n",
        "rating = []\n",
        "feedback = []\n",
        "\n",
        "while True:\n",
        "    # Process the current page\n",
        "    soup = BeautifulSoup(wd.page_source, \"html.parser\")\n",
        "    for parent in soup.find_all(\"div\", class_=\"u-border-general--bottom\"):\n",
        "        anchor = parent.find('a')\n",
        "        if anchor:\n",
        "            second_half = anchor.get('href')\n",
        "            link.append(\"https://www.practo.com/\" + second_half)  # links of all doctors\n",
        "        else:\n",
        "            link.append(None)  # or any other suitable placeholder if a link is not found\n",
        "\n",
        "        doctor_name_element = parent.select(\".doctor-name\")\n",
        "        if doctor_name_element:\n",
        "            doc_name.append(doctor_name_element[0].get_text())  # Doctor name\n",
        "        else:\n",
        "            doc_name.append(\"N/A\")  # Placeholder for missing doctor name\n",
        "\n",
        "\n",
        "        exp_element = parent.select(\".uv2-spacer--xs-top\")\n",
        "        if exp_element:\n",
        "            exp.append(exp_element[0].get_text().split()[0])  # doctor experience\n",
        "        else:\n",
        "            exp.append(\"N/A\")  # Placeholder for missing experience\n",
        "\n",
        "        info_element = parent.find(\"div\", class_=\"info-section\")\n",
        "        if info_element:\n",
        "            info.append(info_element.text)\n",
        "        else:\n",
        "            info.append(\"N/A\")  # Placeholder for missing info\n",
        "\n",
        "        fee_element = parent.find('span', attrs={'data-qa-id': 'consultation_fee'})\n",
        "        if fee_element:\n",
        "            fee.append(fee_element.text)\n",
        "        else:\n",
        "            fee.append(\"N/A\")  # Placeholder for missing fee\n",
        "\n",
        "        locality_element = parent.find('span', attrs={'data-qa-id': 'practice_locality'})\n",
        "        if locality_element:\n",
        "            local.append(locality_element.text)\n",
        "        else:\n",
        "            local.append(\"N/A\")  # Placeholder for missing locality\n",
        "\n",
        "        city_element = parent.find('span', attrs={'data-qa-id': 'practice_city'})\n",
        "        if city_element:\n",
        "            city.append(city_element.text)\n",
        "        else:\n",
        "            city.append(\"N/A\")  # Placeholder for missing city\n",
        "\n",
        "        types_element = parent.find(\"div\", class_=\"u-grey_3-text\")\n",
        "        if types_element:\n",
        "            types_div = types_element.find(\"div\", class_=\"u-d-flex\")\n",
        "            if types_div:\n",
        "                types_span = types_div.span\n",
        "                if types_span:\n",
        "                    types.append(types_span.text)\n",
        "                else:\n",
        "                    types.append(\"N/A\")\n",
        "            else:\n",
        "                types.append(\"N/A\")\n",
        "        else:\n",
        "            types.append(\"N/A\")\n",
        "\n",
        "        rating_element = parent.find('span', attrs={'data-qa-id': 'doctor_recommendation'})\n",
        "        if rating_element:\n",
        "            rating.append(rating_element.text)\n",
        "        else:\n",
        "            rating.append(\"N/A\")  # Placeholder for missing rating\n",
        "\n",
        "        feedback_element = parent.find('span', attrs={'data-qa-id': 'total_feedback'})\n",
        "        if feedback_element:\n",
        "            feedback.append(feedback_element.text)\n",
        "        else:\n",
        "            feedback.append(\"N/A\")\n",
        "\n",
        "    # Check for the presence of the \"Next\" button\n",
        "    next_button = wd.find_element(By.CSS_SELECTOR, \"a[data-qa-id='pagination_next']\")\n",
        "    if 'disabled' in next_button.get_attribute(\"class\"):\n",
        "        break  # No more pages available\n",
        "    else:\n",
        "        # Use JavaScript to click the \"Next\" button\n",
        "        wd.execute_script(\"arguments[0].click();\", next_button)\n",
        "        time.sleep(10)  # Wait for the next page to load\n",
        "\n",
        "# Create a DataFrame from the scraped data\n",
        "data = {\n",
        "    \"Doctor Name\": doc_name,\n",
        "    \"Doctor Profile Link\": link,\n",
        "    \"Experience\": exp,\n",
        "    \"Info\": info,\n",
        "    \"Consultation Fee\": fee,\n",
        "    \"Locality\": local,\n",
        "    \"City\": city,\n",
        "    \"Types\": types,\n",
        "    \"Rating\": rating,\n",
        "    \"Feedback\": feedback\n",
        "}\n",
        "\n",
        "df = pd.DataFrame(data)\n",
        "\n",
        "# Save the DataFrame to a CSV file\n",
        "df.to_csv(\"doctors_data.csv\", index=False)\n",
        "\n",
        "# Close the WebDriver\n",
        "wd.quit()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 250
        },
        "id": "Yy8JTFUxa3l6",
        "outputId": "b1addde4-6eb5-44b2-862d-8d547b7cc681"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-2-f16894cb337a>\u001b[0m in \u001b[0;36m<cell line: 30>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    109\u001b[0m         \u001b[0;31m# Use JavaScript to click the \"Next\" button\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    110\u001b[0m         \u001b[0mwd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecute_script\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"arguments[0].click();\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnext_button\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 111\u001b[0;31m         \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Wait for the next page to load\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    112\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[0;31m# Create a DataFrame from the scraped data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "from selenium import webdriver\n",
        "from bs4 import BeautifulSoup\n",
        "import pandas as pd\n",
        "\n",
        "# List of cities in Jharkhand\n",
        "cities = [\n",
        "    \"ranchi\", \"Bokaro\", \"Deoghar\", \"Hazaribag\", \"Ramgarh\", \"Giridih\",\n",
        "    \"Chatra\", \"Dhanbad\", \"Koderma\", \"Palamu\", \"west-singhbhum\", \"east-singhbhum\",\n",
        "    \"Chaibasa\", \"Dumka\", \"Simdega\", \"pakur\", \"Gumla\", \"Lohardaga\"\n",
        "]\n",
        "\n",
        "options = webdriver.ChromeOptions()\n",
        "options.add_argument('--headless')\n",
        "options.add_argument('--no-sandbox')\n",
        "options.add_argument('--disable-dev-shm-usage')\n",
        "\n",
        "# Create an empty DataFrame to store the scraped data\n",
        "all_data = pd.DataFrame()\n",
        "\n",
        "for city in cities:\n",
        "    # Initialize WebDriver for each city\n",
        "    wd = webdriver.Chrome(options=options)\n",
        "    url = f\"https://www.practo.com/{city}/doctors\"\n",
        "    wd.get(url)\n",
        "    time.sleep(3)  # Allow 3 seconds for the webpage to load\n",
        "\n",
        "    scroll_height = wd.execute_script(\"return document.body.scrollHeight\")\n",
        "    new_height = 0\n",
        "\n",
        "    while True:\n",
        "        wd.execute_script(\"window.scrollTo(0, document.body.scrollHeight-1500);\")  # Scroll to the last page\n",
        "        time.sleep(3)  # Scroll time\n",
        "\n",
        "        new_height = wd.execute_script(\"return document.body.scrollHeight\")\n",
        "\n",
        "        if new_height == scroll_height:\n",
        "            break  # Break the loop if the previous page height is the same as the new height after scrolling and refreshing\n",
        "        scroll_height = new_height  # Assigning\n",
        "\n",
        "    # Scrape data for the current city\n",
        "    urls = []\n",
        "    link = []\n",
        "    doc_name = []\n",
        "    exp = []\n",
        "    info = []\n",
        "    fee = []\n",
        "    types = []\n",
        "    city = []\n",
        "    local = []\n",
        "    rating = []\n",
        "    feedback = []\n",
        "\n",
        "    soup = BeautifulSoup(wd.page_source, \"html.parser\")\n",
        "    for parent in soup.find_all(\"div\", class_=\"u-border-general--bottom\"):\n",
        "        anchor = parent.find('a')\n",
        "        if anchor:\n",
        "            second_half = anchor.get('href')\n",
        "            link.append(\"https://www.practo.com/\" + second_half)  # Links of all doctors\n",
        "        else:\n",
        "            link.append(None)  # Or any other suitable placeholder if a link is not found\n",
        "\n",
        "        doctor_name_element = parent.select(\".doctor-name\")\n",
        "        if doctor_name_element:\n",
        "            doc_name.append(doctor_name_element[0].get_text())  # Doctor name\n",
        "        else:\n",
        "            doc_name.append(\"N/A\")  # Placeholder for missing doctor name\n",
        "\n",
        "        exp_element = parent.select(\".uv2-spacer--xs-top\")\n",
        "        if exp_element:\n",
        "            exp.append(exp_element[0].get_text().split()[0])  # Doctor experience\n",
        "        else:\n",
        "            exp.append(\"N/A\")  # Placeholder for missing experience\n",
        "\n",
        "        info_element = parent.find(\"div\", class_=\"info-section\")\n",
        "        if info_element:\n",
        "            info.append(info_element.text)\n",
        "        else:\n",
        "            info.append(\"N/A\")  # Placeholder for missing info\n",
        "\n",
        "        fee_element = parent.find('span', attrs={'data-qa-id': 'consultation_fee'})\n",
        "        if fee_element:\n",
        "            fee.append(fee_element.text)\n",
        "        else:\n",
        "            fee.append(\"N/A\")  # Placeholder for missing fee\n",
        "\n",
        "        locality_element = parent.find('span', attrs={'data-qa-id': 'practice_locality'})\n",
        "        if locality_element:\n",
        "            local.append(locality_element.text)\n",
        "        else:\n",
        "            local.append(\"N/A\")  # Placeholder for missing locality\n",
        "\n",
        "        city_element = parent.find('span', attrs={'data-qa-id': 'practice_city'})\n",
        "        if city_element:\n",
        "            city.append(city_element.text)\n",
        "        else:\n",
        "            city.append(\"N/A\")  # Placeholder for missing city\n",
        "\n",
        "        types_element = parent.find(\"div\", class_=\"u-grey_3-text\")\n",
        "        if types_element:\n",
        "            types_div = types_element.find(\"div\", class_=\"u-d-flex\")\n",
        "            if types_div:\n",
        "                types_span = types_div.span\n",
        "                if types_span:\n",
        "                    types.append(types_span.text)\n",
        "                else:\n",
        "                    types.append(\"N/A\")\n",
        "            else:\n",
        "                types.append(\"N/A\")\n",
        "        else:\n",
        "            types.append(\"N/A\")\n",
        "\n",
        "        rating_element = parent.find('span', attrs={'data-qa-id': 'doctor_recommendation'})\n",
        "        if rating_element:\n",
        "            rating.append(rating_element.text)\n",
        "        else:\n",
        "            rating.append(\"N/A\")  # Placeholder for missing rating\n",
        "\n",
        "        feedback_element = parent.find('span', attrs={'data-qa-id': 'total_feedback'})\n",
        "        if feedback_element:\n",
        "            feedback.append(feedback_element.text)\n",
        "        else:\n",
        "            feedback.append(\"N/A\")\n",
        "\n",
        "    # Close the WebDriver for the current city\n",
        "    wd.quit()\n",
        "\n",
        "    # Create a DataFrame for the current city's data\n",
        "    city_data = {\n",
        "        \"City\": [city] * len(link),\n",
        "        \"Doctor Name\": doc_name,\n",
        "        \"Doctor Profile Link\": link,\n",
        "        \"Experience\": exp,\n",
        "        \"Info\": info,\n",
        "        \"Consultation Fee\": fee,\n",
        "        \"Locality\": local,\n",
        "        \"City\": city,\n",
        "        \"Types\": types,\n",
        "        \"Rating\": rating,\n",
        "        \"Feedback\": feedback\n",
        "    }\n",
        "\n",
        "    city_df = pd.DataFrame(city_data)\n",
        "\n",
        "    # Append the current city's data to the overall DataFrame\n",
        "    all_data = pd.concat([all_data, city_df], ignore_index=True)\n",
        "\n",
        "# Save the overall DataFrame to a CSV file\n",
        "all_data.to_csv(\"doctors_data_jharkhand.csv\", index=False)\n",
        "\n",
        "print(\"Scraping and data compilation completed.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aG61eoSAOyrH",
        "outputId": "4edcceae-6fe5-4e31-8efe-791dbd39681f"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Scraping and data compilation completed.\n"
          ]
        }
      ]
    }
  ]
}